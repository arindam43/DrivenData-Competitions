BuildModels <- function(model.data, country.name){
  #' @title 
  #' Train and evaluate models on training data
  #' 
  #' @description 
  #' Builds models on training data and evaluates performance using CV.
  #' Only model used here is XGBoost (many other models were tested during the
  #'   competition, but they did not yield any meaningful improvement, to the 
  #'   best of my recollection).
  #'
  #' @param model.data 
  #' list of data.table: list containing train and test data; list names should 
  #'   be "train" and "test" 
  #' @param country.name 
  #' character: country name associated with data/model
  #'
  #' @return
  #' train: train object returned by caret after fitting best model
  #'   ("best" as determined by CV)
  
  log_info(glue("Training and evaluating models for Country {country.name}..."))
  start.time <- proc.time()
  
  adaptive.trcontrol <- trainControl(method = "adaptive_cv",
                                     number = 10,
                                     adaptive = list(min = 5,
                                                     alpha = 0.05,
                                                     method = "gls",
                                                     complete = TRUE),
                                     classProbs = TRUE,
                                     verboseIter = FALSE,
                                     summaryFunction = BinaryClassSummary,
                                     savePredictions = "final")
  
  cv.trcontrol <- trainControl(method = "cv",
                               number = 10,
                               classProbs = TRUE,
                               verboseIter = TRUE,
                               summaryFunction = BinaryClassSummary,
                               savePredictions = "final")
  
  
  xgb.grid <- expand.grid(nrounds = 750,
                          eta = c(0.02, 0.04, 0.06),
                          max_depth = 3,
                          gamma = 0,
                          colsample_bytree = 0.5,
                          min_child_weight = c(1, 3, 5),
                          subsample = 1)
  
  xgb.model <- BuildXGBoostModel(model.data$train,
                                 response = 'hh.poor',
                                 grid = xgb.grid,
                                 trcontrol = cv.trcontrol)
  
  duration <- round(proc.time() - start.time, 2)
  log_info("Model training complete in {duration[['elapsed']]} seconds.")

  return(xgb.model)
  }
  

BuildXGBoostModel <- function(data, response, grid, trcontrol, ...) {
  #' @title 
  #' Build XGBoost model
  #' 
  #' @description
  #' Trains XGBoost model via caret package.
  #'
  #' @param data 
  #' data.table: data.table containing all training data (features + response)
  #' @param response 
  #' character: string containing name of response column
  #' @param grid 
  #' list: grid for CV created by expand.grid() from caret package
  #' @param trcontrol 
  #' list: training parameters (e.g., holdout method, # of CV folds, etc.)
  #'   generated by trainControl from caret package
  #' @param ... 
  #' additional parameters to pass to caret's train() function for XGBoost
  #'
  #' @return
  #' train: train object returned by caret after training and tuning a model    
  
  set.seed(1)
  
  xgb.fit <- train(x = data[, .SD, .SDcols = !c(response, 'id')],
                   y = data[[response]],
                   method = "xgbTree",
                   metric = "LogLoss",
                   tuneGrid = grid,
                   trControl = trcontrol,
                   maximize = FALSE,
                   ...)
  
  return(xgb.fit)
}
